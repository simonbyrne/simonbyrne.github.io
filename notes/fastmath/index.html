<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/libs/highlight/github.min.css"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/basic.css"> <link rel=icon  href="/assets/favicon.png"> <title>Beware of fast-math</title> <header> <div class=blog-name ><a href="/">Simon's notes</a></div> <nav> <ul> <li><a href="/">Home</a> <li><a href="/about/">About me</a> </ul> <img src="/assets/hamburger.svg" id=menu-icon > </nav> </header> <div class=franklin-content ><h1 id=title ><a href="#title" class=header-anchor >Beware of fast-math</a></h1> <p>One of my more frequent rants, both online and in person, is the danger posed by the &quot;fast-math&quot; compiler flag. While these rants may elicit resigned acknowledgment from those who already understand the dangers involved, they do little to help those who don&#39;t. So given the remarkable paucity of writing on the topic &#40;including the documentation of the compilers themselves&#41;, I decided it would make a good inaugural topic for this blog.</p> <h2 id=so_what_is_fast-math ><a href="#so_what_is_fast-math" class=header-anchor >So what is fast-math?</a></h2> <p>It&#39;s a compiler flag or option that exists in many languages and compilers, including:</p> <ul> <li><p><code>-ffast-math</code> &#40;and included by <code>-Ofast</code>&#41; in <a href="https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html">GCC</a> and <a href="https://clang.llvm.org/docs/UsersManual.html#cmdoption-ffast-math">Clang</a></p> <li><p><code>-fp-model&#61;fast</code> in <a href="https://www.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/compiler-reference/compiler-options/compiler-option-details/floating-point-options/fp-model-fp.html">ICC</a></p> <li><p><code>/fp:fast</code> in <a href="https://docs.microsoft.com/en-us/cpp/build/reference/fp-specify-floating-point-behavior?view&#61;msvc-170">MSVC</a></p> <li><p><a href="https://docs.julialang.org/en/v1/manual/command-line-options/#command-line-options"><code>--math-mode&#61;fast</code> command line option</a> or <a href="https://docs.julialang.org/en/v1/base/math/#Base.FastMath.@fastmath"><code>@fastmath</code> macro</a> in Julia.</p> </ul> <p>So what does it actually do? Well, as the name said, it makes your math faster. That sounds great, we should definitely do that&#33;</p> <blockquote> <p>I mean, the whole point of fast-math is trading off speed with correctness. If fast-math was to give always the correct results, it wouldn’t be fast-math, it would be the standard way of doing math.</p> </blockquote> <p>&mdash; <a href="https://discourse.julialang.org/t/whats-going-on-with-exp-and-math-mode-fast/64619/7?u&#61;simonbyrne">Mosè Giordano</a></p> <p>The rules of floating point operations are specified in <a href="https://en.wikipedia.org/wiki/IEEE_754">the IEEE 754 standard</a>, which all popular programming languages &#40;mostly&#41; adhere to; compilers are only allowed to perform optimizations which obey these rules. Fast-math allows the compiler to break some of these rules: these breakages may seem pretty innocuous at first glance, but can have significant and occasionally unfortunate downstream effects.</p> <p>In <a href="https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html">GCC</a>, <code>-ffast-math</code> &#40;or <code>-Ofast</code>&#41; enables the following options: <code>-fno-math-errno</code>, <code>-funsafe-math-optimizations</code>, <code>-ffinite-math-only</code>, <code>-fno-rounding-math</code>, <code>-fno-signaling-nans</code>, <code>-fcx-limited-range</code> and <code>-fexcess-precision&#61;fast</code>. Note that <code>-funsafe-math-optimizations</code> is itself a collection of options <code>-fno-signed-zeros</code>, <code>-fno-trapping-math</code>, <code>-fassociative-math</code> and <code>-freciprocal-math</code>, plus some extra ones, which we will discuss further below.</p> <p>Now some of these are unlikely to cause problems in most cases: <code>-fno-math-errno</code><sup id="fnref:1"><a href="#fndef:1" class=fnref >[1]</a></sup>, <code>-fno-signaling-nans</code>, <code>-fno-trapping-math</code> disable rarely-used &#40;and poorly supported&#41; features. Others, such as <code>-freciprocal-math</code> can reduce accuracy slightly, but are unlikely to cause problems in most cases.</p> <p><a href="https://kristerw.github.io/2021/10/19/fast-math/">Krister Walfridsson</a> gives a very nice &#40;and somewhat more objective&#41; description of some of these, but I want to focus three in particular.</p> <h2 id=a_hrefhttpsgccgnuorgonlinedocsgccoptimize-optionshtmlindex-ffinite-math-only-ffinite-math-only ><a href="#a_hrefhttpsgccgnuorgonlinedocsgccoptimize-optionshtmlindex-ffinite-math-only-ffinite-math-only" class=header-anchor ><a href="https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html#index-ffinite-math-only"><code>-ffinite-math-only</code></a></a></h2> <blockquote> <p>Allow optimizations for floating-point arithmetic that assume that arguments and results are not NaNs or &#43;-Infs.</p> </blockquote> <p>The intention here is to allow the compiler to perform some <a href="https://stackoverflow.com/a/10145714/392585">extra optimizations</a> that would not be correct if NaNs or Infs were present: for example the condition <code>x &#61;&#61; x</code> can be assumed to always be true &#40;it evaluates false if <code>x</code> is a NaN&#41;.</p> <p>This sounds great&#33; My code doesn&#39;t generate any NaNs or Infs, so this shouldn&#39;t cause any problems.</p> <p>But what if your code does generate any intermediate NaNs only because it internally calls <code>isnan</code> to ensure that they are correctly handled?</p> <p> <iframe width="100%" height=400px  src="https://gcc.godbolt.org/e#z:OYLghAFBqd5QCxAYwPYBMCmBRdBLAF1QCcAaPECAMzwBtMA7AQwFtMQByARg9KtQYEAysib0QXACx8BBAKoBnTAAUAHpwAMvAFYTStJg1AB9U8lJL6yAngGVG6AMKpaAVxYMQAJlIOAMngMmABy7gBGmMQg0gAOqAqEtgzObh7epHEJNgIBQaEsEVHSlpjWSUIETMQEKe6ePiVlAhVVBLkh4ZHRFpXVtWkNvW2BHQVdkgCUFqiuxMjsHACkXgDMgchuWADUiyuOLEwECAB0CLvYixoAgoEEW/yo1LSoh/cTOwDsAEKXV1v/W2ImAIswYWzwCmYDGoE12P2uiw%2BABEOFNaJwAKy8TwcLSkVCcRxbBQzOaYHarHikAiaVFTADW0Q%2Bxw%2BAE4AByrdkY1kadkfLwffScSS8FgSDQaUg4vEEji8BQgKU03Go0hwWAwRAoVAsGJ0SLkShoPUGqLIYBcLg%2BGi0AiRRUQMK00hhQJVACenCpJrYggA8gxaF7VaQsAcjOJQ/ggWUAG6YRWhzCqUque3e3i3TDo0O0PBhYie5xYF0EYh4cXcNVUAzABQANTwmAA7v6YoxMzJBCIxOwpN35Eo1C7dFx9IYTGZ9AXFZApqgYtkGEmALRUKhMBQEVcHI5bVf%2BlYKnOlZf2BhOFx1CQ%2BfwjfKFEArDLxRICfqea2vrJJdqProX0aZcWj6a80m/YDyiGf9OiiIChk/W8elaWCxngqYSVmeY9HLTAFh4NFMWxF05VUdkADZVwoyQtmAZBkC2K1ji8LYIEcUgtlwQgSApFZxy2ZxTXoYg%2BK4CZeBVLQJimBBMCYLAoggBkQAxKVc1FUhxTU6VSM4BUlWpWkpg1bUTX1ESjQgcyzRQDZJy4FZJT4Oh7WIR1nVDN1mGIEMfV1P0CEDYMXXDScozxGMzzwBMkzxFM0wzatyEEHMXXzQtiwwBY8XLStMymWsmHrJtW3bTtkv4HtRHEAcqqHFR1FDXQfAMIwQFMYxzAyudlPxJckjXDctx3PcEAPI8Tysc8IAcJDx3vPI4L0TJ32ScCvx/Na0KfccoOaRCNr0faGFA4YlvQ47DtSTbt1Qh9lvE6ZsP7akgQItVcyxXTQzIyjqNo%2ByjCYlZjg0MG2I4rj8CIUTln4zihIsyI%2BK8CSjNVGTSDkhSuj6jSxVUqUZV4OUDOVYyVIxLwWI0LgPgoqQNFZa0PhfXNjx%2B2V9Ix6SiI4LwSN%2BnmpLpUgE3cpJoiAA%3D"></iframe> &mdash; based on <a href="https://twitter.com/johnregehr/status/1440024236257542147">an example from John Regehr</a></p> <p>&#40;to explain what this is showing: the function is setting the return register <code>eax</code> to zero, by <code>xor</code>-ing it with itself, which means the function will always return <code>false</code>&#41;</p> <p>That&#39;s right, your compiler has just removed all those checks.</p> <p>Depending on who you ask, this is either obvious &#40;&quot;you told the compiler there were no NaNs, so why does it need to check?&quot;&#41; or ridiculous &#40;&quot;how can we safely optimize away NaNs if we can&#39;t check for them?&quot;&#41;. Even compiler developers <a href="https://twitter.com/johnregehr/status/1440021297103134720">can&#39;t agree</a>.</p> <p>This is perhaps the single most frequent cause of fast-math-related <a href="https://stackoverflow.com/a/22931368/392585 ">StackOverflow</a> <a href="https://stackoverflow.com/q/7263404/392585">questions</a> and <a href="https://github.com/numba/numba/issues/2919">GitHub</a> <a href="https://github.com/google/jax/issues/276 ">bug</a> <a href="https://github.com/pytorch/glow/issues/2073">reports</a>, and so if your fast-math-compiled code is giving wrong results, the very first thing you should do is disable this option &#40;<code>-fno-finite-math-only</code>&#41;.</p> <h2 id=a_hrefhttpsgccgnuorgonlinedocsgccoptimize-optionshtmlindex-fassociative-math-fassociative-math ><a href="#a_hrefhttpsgccgnuorgonlinedocsgccoptimize-optionshtmlindex-fassociative-math-fassociative-math" class=header-anchor ><a href="https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html#index-fassociative-math"><code>-fassociative-math</code></a></a></h2> <blockquote> <p>Allow re-association of operands in series of floating-point operations.</p> </blockquote> <p>This allows the compiler to change the order of evaluation in a sequence of floating point operations. For example if you have an expression <code>&#40;a &#43; b&#41; &#43; c</code>, it can evaluate it instead as <code>a &#43; &#40;b &#43; c&#41;</code>. While these are mathematically equivalent with real numbers, they aren&#39;t equivalent in floating point arithmetic: the errors they incur can be different, in some cases quite significantly so:</p> <pre><code class="julia hljs">julia&gt; a = <span class=hljs-number >1e9</span>+<span class=hljs-number >1</span>; b = -<span class=hljs-number >1e9</span>; c = <span class=hljs-number >0.1</span>;

julia&gt; (a+b)+c
<span class=hljs-number >1.1</span>

julia&gt; a+(b+c)
<span class=hljs-number >1.100000023841858</span></code></pre> <h3 id=vectorization ><a href="#vectorization" class=header-anchor >Vectorization </a></h3> <p>So why would you want to do this? One primary reason is that it can enable use of vector/SIMD instructions:</p> <iframe width="100%" height=400px  src="https://gcc.godbolt.org/e#z:OYLghAFBqd5QCxAYwPYBMCmBRdBLAF1QCcAaPECAMzwBtMA7AQwFtMQByARg9KtQYEAysib0QXACx8BBAKoBnTAAUAHpwAMvAFYTStJg1AB9U8lJL6yAngGVG6AMKpaAVxYMQAJlIOAMngMmABy7gBGmMQSXKQADqgKhLYMzm4e3nEJSQIBQaEsEVFcMZaY1slCBEzEBKnunj6l5QKV1QS5IeGR0RZVNXXpjX3tgZ0F3cUAlBaorsTI7BwApF4AzIHIblgA1EurjixMBAgAdAh72EsaAIJX11S0qEfbCu5eAKwAbNSPzwBU22qxEmuwA7AAhO7baHbB5PAgvPaQm4w7aBBF4JFQmEKXarAAi2w0Jw0VCxKJh/GI2wgeDxhI0SLReMc2w%2BnyZmK8kO52xBSwh2NRuL2hJFvKBS3e4OZUsJTKFAvxQuImAIcwYiNWyNuoPxHGmtE4714ng4WlIqE4rIUs3mmF2ax4pAImgN0wA1iBJKCTqCAJwADjWgfe/o0gdBXlB%2Bk4kl4LAkGg0pDNFqtHF4ChAKdd5oNpDgsBgiBQqBYsTokXIlDQFarUWQwGKPhotAIkWzEDCbtIYUC1QAnpxnXW2IIAPIMWjD/OkLCHIziOf4VXlABumGzc8wqjKrg7I946MwRrntDwYWIQ%2BcWF7BGIeET3ALDyYwAUADU8JgAO4T2JGCPGRBBEMR2CkED5CUNRe10GIDCMEBTGMcwLzCbNIGmVBYhsARtwAWgnVYs1PMo8M8CAHAGTwYn8UZ8kKPR4kSCiaOYrIKI6RiJgsMjmgYVp%2Bhceo9CaCihJGPIuiKXo2nYkphm4mSJGmW05gWPQH0wRYeENY1TV7DNVEDT4CM%2BSRtmAZBkG2YoTi8GlHFIbZcEIEhHVWGJtmcet6GpFYvMmXg8y0SZpgQTAmCwKIIE9EB3hTM941IRNEtTIzOCzHMXTdaYi1LOtK38msICKhsUGbLhWzoDtiC7Hs537ZhiFnUdy3HAgpxnXsF0MYBlwtVdyLwTdtwtXd90PF9yEEU9e3Q69WtvRYLQfJ8j2mN8P2/P8AKAmb%2BFA0RxEgo7oJUdQ510HxEJMMx9EvTC4stXDkkI4jtgIqgqCYBQCAIw5jlIqwKPsBgnBE9I6Ih5TxlkljshSKHaMyVjkjhpiSn4iThgUvjQYqJSGJUxT5JRsTiek%2BHVJmDSIJdVVdILM8TQyudjNM8zLOs2z7MciBnNc/AiACp0XN84rIk8rxgty/NwtISLou6F7koTBKUzTXgM2y3M8vi94vAcjQuFBT4pA0f1qtBVZYw4Ej2fTLL5bC/SOC8QyOZd0L3VITd6uSb0gA"></iframe> <p>For those who aren&#39;t familiar with SIMD operations &#40;or reading assembly&#41;, I&#39;ll try to explain briefly what is going on here &#40;others can skip this part&#41;. Since raw clock speeds haven&#39;t been getting much faster, one way in which processors have been able to increase performance is through operations which operate on a &quot;vector&quot; &#40;basically, a short sequence of values contiguous in memory&#41;.</p> <p>In this case, instead of performing a sequence of floating point additions &#40;<code>addss</code>&#41;, it is able to make use of a SIMD instruction &#40;<code>addps</code>&#41; which takes vector <code>float</code>s &#40;4 in this case, but it can be up to 16 with AVX 512 instructions&#41;, and adds them element-wise to another vector in one operation. It does this for the whole array, followed by a final reduction step where to sum the vector to a single value. This means that instead of evaluating</p> <pre><code class="julia hljs">s = arr[<span class=hljs-number >0</span>] + arr[<span class=hljs-number >1</span>];
s = s + arr[<span class=hljs-number >2</span>];
s = s + arr[<span class=hljs-number >3</span>];
...
s = s + arr[<span class=hljs-number >255</span>];</code></pre> <p>it is actually doing</p> <pre><code class="julia hljs">s0 = arr[<span class=hljs-number >0</span>] + arr[<span class=hljs-number >4</span>]; s1 = arr[<span class=hljs-number >1</span>] + arr[<span class=hljs-number >5</span>]; s2 = arr[<span class=hljs-number >2</span>] + arr[<span class=hljs-number >6</span>];  s3 = arr[<span class=hljs-number >3</span>] + arr[<span class=hljs-number >7</span>];
s0 = s0 + arr[<span class=hljs-number >8</span>];     s1 = s1 + arr[<span class=hljs-number >9</span>];     s2 = s2 + arr[<span class=hljs-number >10</span>];     s3 = s3 + arr[<span class=hljs-number >11</span>]);
...
s0 = s0 + arr[<span class=hljs-number >252</span>];   s1 = s1 + arr[<span class=hljs-number >253</span>];   s2 = s2 + arr[<span class=hljs-number >254</span>];    s3 = s3 + arr[<span class=hljs-number >255</span>]);
sa = s0 + s1;
sb = s2 + s3;
s = sa + sb;</code></pre> <p>where each line corresponds to one floating point instruction.</p> <p>The problem here is that the compiler generally isn&#39;t allowed to make this optimization: it requires evaluating the sum in a different association grouping than was specified in the code, and so can give different results<sup id="fnref:4"><a href="#fndef:4" class=fnref >[2]</a></sup>. Though in this case it is likely harmless &#40;or may even improve accuracy<sup id="fnref:2"><a href="#fndef:2" class=fnref >[3]</a></sup>&#41;, this is not always the case.</p> <h3 id=compensated_arithmetic ><a href="#compensated_arithmetic" class=header-anchor >Compensated arithmetic</a></h3> <p>Certain algorithms however depend very strictly on the order in which floating point operations are performed. In particular <em>compensated arithmetic</em> operations make use of it to compute the error that is incurred in intermediate calculations, and correct for that in later computations.</p> <p>The most well-known algorithm which makes use of this is <a href="https://en.wikipedia.org/wiki/Kahan_summation_algorithm">Kahan summation</a>, which corrects for the round off error incurred at addition step in the summation loop. We can compile an implementation of Kahan summation with <code>-ffast-math</code>, and compare the result to the simple loop summation above:</p> <iframe width="100%" height=600px  src="https://gcc.godbolt.org/e#z:OYLghAFBqd5TKALEBjA9gEwKYFFMCWALugE4A0BIEAZgQDbYB2AhgLbYgDkAjF%2BTXRMiAZVQtGIHgBYBQogFUAztgAKAD24AGfgCsp5eiyagA%2BudTkVjVEQJDqzTAGF09AK5smUgMzknADIETNgAcp4ARtikIABM5AAO6ErE9kyuHl6%2BicmpQkEh4WxRMfHW2LZpIkQspEQZnt48fuWVQtW1RAVhkdFxVjV1DVnNA53dRSVxAJRW6O6kqJxcNPToLEQA1EqetGsbmwBUm7Wk05sApADsAEIXWgCCm8%2Bbq%2BtbShc%2Bd48vm8FbAhfH5PF5KS4%2BAAimy0ADotDRgfdQc9BKRNhACBDoVpgf8Ic5NrEAKwANjxQNidypm3O1xBf2e4K%2B0OZNNOF2JN3xnOheORL2ukIFz1I2CICyY2yRjyFyK4s3o3GJ/G8XB05HQ3EJSnmi2wl1iPj45CI2gVswA1iBiVpDNxpPw2Da7WqNVquPwlCA7Wb1QryHBYCgMGwEgxopRqKHw4wYqhgDwePE6PQiNFvRAIubyBFgrUAJ7cE2hjjCADyTHoRf95BwbGMwEktcIYsqADdsN7a9h1BV3Oni/wAdglbX6AQIqRC64cDmiKQCM7eAHVixgEoAGoEbAAd3LCWYQ7kwjEEk4MhPihUGhz%2Bh4hkbIHMpksE4i3sgs3QCTsQm7AC05Y%2BJsAE0DQLBKEQAENkQSBeqOFR/t4EBOMMTT%2BEwmATL0MQPkkKTIehBgEXkTA4cUfQPq0yEdEMbiNAYNFVIMXTBD0lF4WM9GZBhUHjOxkxUbMuoLEsBgLtgyx8IqyqqjmHrqAAHKSAGktImzAKgqCbEmsKxBizjkJs%2BDEGQhrGsZrhhhG6IXEaPDTPwfo6NMsxINgLA4DEEBWi69pcI65BuvwHpej6prmm5AWxE6IA%2BNIsKkjwWjSFcJLSDwtpXFofghZq3DOVFgaIEGyBoFgeCECQFBULQEasBwx6CKe4iSJeLXXmomi1vo8RGCYz4WHMYnLG8Bw7GwpiWiwSDGHs7xHCcpBnJctwiqi%2BwfMZlibEQxkFjKKL/MI/xHX8zJQpsOksjC8KIt8G2vOZmLYjCFIEkSZIUvZ1LcnS62/IyBZvRyXJAsS0IAdd52Mlst1styh2PUDfw3VdEBbNDSjnNDyMMoyl3QkQsNrcKqNihKpBSp8KMPHKjwBmOKrBQp2rbHqSwWbFkX%2BtFHleX0vnkNatoBUFzo%2BFcsLEqztZhVYEUuQGZUQCG6A2XGUYQDGtloImyYCAw6akJm2a1nmrCkDWJYa2WRCVtWOb1o2zYaq2SEEJ23Yar2/aDiulDCKOObvtO1uzssGoLkuQ6zGuG7bnuB5HoHnVnu1sidco3V3v0A1mMN76fsLP7IYBwGgeBkHQbB8FWIhbQoWhDEjJh2GCbhJG5ERrcYaRyEUVM1GN7RrHESPNhjwJhRd9R4990xrFD8JI36v0knSUzcly%2B63DKap6madpuk8PphnGaZNXc1ZGuxtE3NObzrnuZ53nUH5YtjkF%2BUK96vpRVklwHmktYjwjJDwAAnJlEkWVIHqV3qFQqz8ValRQIQcC2t05tQvFneQOdbzjiQN6B89BiEYJoEQAsh4IqkGIf0OhSgKFUJoVoIBLNf7cEhAQcCmwk67gfgfNSGktI6T0gZds4IhFH1EaffSRU%2BZAIlvFHwsIfDqI0ZozRiCCqekVgAxRIspBaDhCYsx5jzGkO4D4eS8tkHK2ip2U2aQQDSCAA"></iframe> <p>It gives <em>exactly</em> the same assembly as the original summation code above. Why?</p> <p>If you substitute the expression for <code>t</code> into <code>c</code>, you get</p> <pre><code class="C hljs">c = ((s + y) - s) - y);</code></pre>
<p>and by applying reassociation, the compiler will then determine that <code>c</code> is in fact always zero, and so may be completely removed. Following this logic further, <code>y &#61; arr&#91;i&#93;</code> and so the inside of the loop is simply</p>
<pre><code class="C hljs">s = s + arr[i];</code></pre>
<p>and hence it &quot;optimizes&quot; identically to the simple summation loop above.</p>
<p>This might seem like a minor tradeoff, but compensated arithmetic is often used to implement core math functions, such as trigonometric and exponential functions. Allowing the compiler to reassociate inside these can give <a href="https://github.com/JuliaLang/julia/issues/30073#issuecomment-439707503">catastrophically wrong answers</a>.</p>
<h2 id=flushing_subnormals_to_zero ><a href="#flushing_subnormals_to_zero" class=header-anchor >Flushing subnormals to zero</a></h2>
<p>This one is the most subtle, but by far the most insidious, as it can affect code compiled <em>without</em> fast-math, and is only cryptically documented under <code>-funsafe-math-optimizations</code>:</p>
<blockquote>
<p>When used at link time, it may include libraries or startup files that change the default FPU control word or other similar optimizations.</p>
</blockquote>
<p>So what does that mean? Well this is referring to one of those slightly annoying edge cases of floating point numbers, <em>subnormals</em> &#40;sometimes called <em>denormals</em>&#41;. <a href="https://en.wikipedia.org/wiki/Subnormal_number">Wikipedia gives a decent overview</a>, but for our purposes the main thing you need to know is &#40;a&#41; they&#39;re <em>very</em> close to zero, and &#40;b&#41; when encountered, they can incur a significant performance penalty on many processors<sup id="fnref:6"><a href="#fndef:6" class=fnref >[4]</a></sup>.</p>
<p>A simple solution to this problem is &quot;flush to zero&quot; &#40;FTZ&#41;: that is, if a result would return a subnormal value, return zero instead. This is actually fine for a lot of use cases, and this setting is commonly used in audio and graphics applications. But there are plenty of use cases where it isn&#39;t fine: FTZ breaks some important floating point error analysis results, such as <a href="https://en.wikipedia.org/wiki/Sterbenz_lemma">Sterbenz&#39; Lemma</a>, and so unexpected results &#40;such as iterative algorithms failing to converge&#41; may occur.</p>
<p>The problem is how FTZ actually implemented on most hardware: it is not set per-instruction, but instead <a href="https://software.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/compiler-reference/floating-point-operations/understanding-floating-point-operations/setting-the-ftz-and-daz-flags.html">controlled by the floating point environment</a>: more specifically, it is controlled by the floating point control register, which on most systems is set at the thread level: enabling FTZ will affect all other operations in the same thread.</p>
<p>GCC with <code>-funsafe-math-optimizations</code> enables FTZ &#40;and its close relation, denormals-are-zero, or DAZ&#41;, even when building shared libraries. That means simply loading a shared library can change the results in completely unrelated code, which is <a href="https://github.com/JuliaCI/BaseBenchmarks.jl/issues/253#issuecomment-573589022">a fun debugging experience</a>.</p>
<h2 id=what_can_programmers_do ><a href="#what_can_programmers_do" class=header-anchor >What can programmers do?</a></h2>
<p>I&#39;ve joked on Twitter that &quot;friends don&#39;t let friends use fast-math&quot;, but with the luxury of a longer format, I will concede that it has valid use cases, and can actually give valuable performance improvements; as SIMD lanes get wider and instructions get fancier, the value of these optimizations will only increase. At the very least, it can provide a useful reference for what performance is left on the table. So when and how can it be safely used?</p>
<p>One reason is if you don&#39;t care about the accuracy of the results: I come from a scientific computing background where the primary output of a program is a bunch of numbers. But floating point arithmetic is used in many domains where that is not the case, such as audio, graphics, games, and machine learning. I&#39;m not particularly familiar with requirements in these domains, but there is an interesting rant by <a href="https://gcc.gnu.org/legacy-ml/gcc/2001-07/msg02150.html">Linus Torvalds from 20 years ago</a>, arguing that overly strict floating point semantics are of little importance outside scientific domains. Nevertheless, <a href="https://twitter.com/supahvee1234/status/1382907921848221698">some anecdotes</a> suggest fast-math can cause problems, so it is probably still useful understand what it does and why. If you work in these areas, I would love to hear about your experiences, especially if you identified which of these optimizations had a positive or negative impact.</p>
<blockquote>
<p>I hold that in general it’s simply intractable to “defensively” code against the transformations that <code>-ffast-math</code> may or may not perform. If a sufficiently advanced compiler is indistinguishable from an adversary, then giving the compiler access to <code>-ffast-math</code> is gifting that enemy nukes. That doesn’t mean you can’t use it&#33; You just have to test enough to gain confidence that no bombs go off with your compiler on your system.</p>
</blockquote>
<p>&mdash; <a href="https://discourse.julialang.org/t/when-if-a-b-x-1-a-b-divides-by-zero/7154/5?u&#61;simonbyrne">Matt Bauman</a></p>
<p>If you do care about the accuracy of the results, then you need to approach fast-math much more carefully and warily. A common approach is to enable fast-math everywhere, observe erroneous results, and then attempt to isolate and fix the cause as one would usually approach a bug. Unfortunately this task is not so simple: you can&#39;t insert branches to check for NaNs or Infs &#40;the compiler will just remove them&#41;, you can&#39;t rely on a debugger because <a href="https://gitlab.com/libeigen/eigen/-/issues/1674#note_709679831">the bug may disappear in debug builds</a>, and it can even <a href="https://bugzilla.redhat.com/show_bug.cgi?id&#61;1127544">break printing</a>.</p>
<p>So you have to approach fast-math much more carefully. A typical process might be:</p>
<ol>
<li><p>Develop reliable validation tests</p>

<li><p>Develop useful benchmarks</p>

<li><p>Enable fast-math and compare benchmark results</p>

<li><p>Selectively enable/disable fast-math optimizations<sup id="fnref:5"><a href="#fndef:5" class=fnref >[5]</a></sup> to identify</p>
<p>a. which optimizations have a performance impact,</p>
<p>b. which cause problems, and</p>
<p>c. where in the code those changes arise.</p>

<li><p>Validate the final numeric results</p>

</ol>
<p>The aim of this process should be to use the absolute minimum number of fast-math options, in the minimum number of places, while testing to ensure that the places where the optimizations are used remain correct.</p>
<p>Alternatively, you can look into other approaches to achieve the same performance benefits: in some cases it is possible to rewrite the code to achieve the same results: for example, it is not uncommon to see expressions like <code>x * &#40;1/y&#41;</code> in many scientific codebases.</p>
<p>For SIMD operations, tools such as <a href="https://www.openmp.org/spec-html/5.0/openmpsu42.html">OpenMP</a> or <a href="https://ispc.github.io/">ISPC</a> provide constructions to write code that is amenable to automatic SIMD optimizations. Julia provides the <a href="https://docs.julialang.org/en/v1/base/base/#Base.SimdLoop.@simd"><code>@simd</code> macro</a>, though this also has some important caveats on its use. At the more extreme end, you can use <a href="https://stackoverflow.blog/2020/07/08/improving-performance-with-simd-intrinsics-in-three-use-cases/">SIMD intrinsics</a>: these are commonly used in libraries, often with the help of code generation &#40;<a href="http://fftw.org/">FFTW</a> uses this appraoch&#41;, but requires considerably more effort and expertise, and can be difficult to port to new platforms.</p>
<p>Finally, if you&#39;re writing an open source library, please don&#39;t <a href="https://github.com/tesseract-ocr/tesseract/blob/5884036ecdb2807419cbd21b7ca44b630f547d80/Makefile.am#L140">hardcode fast-math into your Makefile</a>.</p>
<h2 id=what_can_language_and_compilers_developers_do ><a href="#what_can_language_and_compilers_developers_do" class=header-anchor >What can language and compilers developers do?</a></h2>
<p>I think the widespread use of fast-math should be considered a fundamental design failure: by failing to provide programmers with features they need to make the best use of modern hardware, programmers instead resort to enabling an option that is known to be blatantly unsafe.</p>
<p>Firstly, GCC should address the FTZ library issue: the bug has been <a href="https://gcc.gnu.org/bugzilla/show_bug.cgi?id&#61;55522">open for 9 years, but is still marked NEW</a>. At the very least, this behavior should be more clearly documented, and have a specific option to disable it.</p>
<p>Beyond that, there are 2 primary approaches: educate users, and provide finer control over the optimizations.</p>
<p>The easiest way to educate users is to give it a better name. Rather than &quot;fast-math&quot;, something like &quot;unsafe-math&quot;. Documentation could also be improved to educate users on the consequences of these choices &#40;consider this post to be my contribution to toward that goal&#41;. Linters and compiler warnings could, for example, warn users that their <code>isnan</code> checks are now useless, or even just highlight which regions of code have been impacted by the optimizations.</p>
<p>Secondly, languages and compilers need to provide better tools to get the job done. Ideally these behaviors shouldn&#39;t be enabled or disabled via a compiler flag, which is a very blunt tool, but specified locally in the code itself, for example</p>
<ul>
<li><p>Both GCC and Clang let you <a href="https://stackoverflow.com/a/40702790/392585">enable/disable optimizations on a per-function basis</a>: these should be standardized to work with all compilers.</p>

<li><p>There should be options for even finer control, such as a pragma or macro so that users can assert that &quot;under no circumstances should this <code>isnan</code> check be removed/this arithmetic expression be reassociated&quot;.</p>

<li><p>Conversely, a mechanism to flag certain addition or subtraction operations which the compiler is allowed to reassociate &#40;or contract into a fused-multiply-add operation&#41; regardless of compiler flags.<sup id="fnref:3"><a href="#fndef:3" class=fnref >[6]</a></sup></p>

</ul>
<p>This still leaves open the exact question of what the semantics should be: if you combine a regular <code>&#43;</code> and a fast-math <code>&#43;</code>, can they reassociate? What should the scoping rules be, and how should it interact with things like inter-procedural optimization? These are hard yet very important questions, but they need to be answered for programmers to be able to make use of these features safely.</p>
<p><table class=fndef  id="fndef:1">
    <tr>
        <td class=fndef-backref ><a href="#fnref:1">[1]</a>
        <td class=fndef-content >Apparently <code>-fno-math-errno</code> in GCC <a href="https://twitter.com/kwalfridsson/status/1450556903994675205">can affect <code>malloc</code></a>, so may not be quite so harmless.
    
</table>
 <table class=fndef  id="fndef:4">
    <tr>
        <td class=fndef-backref ><a href="#fnref:4">[2]</a>
        <td class=fndef-content >In fact, it possible to construct array such that taking the sum in different ways can produce <a href="https://discourse.julialang.org/t/array-ordering-and-naive-summation/1929?u&#61;simonbyrne">almost any floating point value</a>.
    
</table>
 <table class=fndef  id="fndef:2">
    <tr>
        <td class=fndef-backref ><a href="#fnref:2">[3]</a>
        <td class=fndef-content >One important result in numerical analysis is that <a href="https://www.google.com/books/edition/Accuracy_and_Stability_of_Numerical_Algo/5tv3HdF-0N8C?hl&#61;en&amp;gbpv&#61;1&amp;pg&#61;PA82&amp;printsec&#61;frontcover">the error bound on summation is proportional to the sum of the absolute values of the intermediate sums</a>. SIMD summation splits the accumulation over multiple values, so will typically give smaller intermediate sums.
    
</table>
 <table class=fndef  id="fndef:6">
    <tr>
        <td class=fndef-backref ><a href="#fnref:6">[4]</a>
        <td class=fndef-content ><a href="https://stackoverflow.com/a/54938328">A good description of why subnormals incur performance penalties</a>.
    
</table>
 <table class=fndef  id="fndef:5">
    <tr>
        <td class=fndef-backref ><a href="#fnref:5">[5]</a>
        <td class=fndef-content >As mentioned above, <code>-fno-finite-math-only</code> should be the first thing you try.
    
</table>
 <table class=fndef  id="fndef:3">
    <tr>
        <td class=fndef-backref ><a href="#fnref:3">[6]</a>
        <td class=fndef-content >Rust provides something like this via <a href="https://stackoverflow.com/a/40707111/392585">experimental intrinsics</a>, though I&#39;m not 100&#37; clear on what optimzations are allowed.
    
</table>
</p>
<div class=page-foot >
  <div class=copyright >
    &copy; Simon Byrne. Last modified: November 12, 2021.
  </div>
</div>
</div>